{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "t5-summarizer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPOJ7rFVFQ2w5N0u3sHUEA8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ACM-Research/NLP-Summarizer/blob/master/Notebooks/t5-summarizer/t5_summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_a8ugdFxnea1"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbvL7WgTnqMu"
      },
      "source": [
        "code originally gotten from\n",
        "https://towardsdatascience.com/simple-abstractive-text-summarization-with-pretrained-t5-text-to-text-transfer-transformer-10f6d602c426"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCBpqOvGta6h",
        "outputId": "05194c2c-65bc-4649-ef82-4314baf39842",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJT1vo7tsdmp",
        "outputId": "495d977d-a0d2-4d14-d6ab-403a7584b922",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "import json \n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n",
        "import os,sys\n",
        "import json\n",
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "#T5 model Types\n",
        "#\n",
        "#t5-small\n",
        "#t5-base\n",
        "#t5-large\n",
        "#t5-3B\n",
        "#t5-11B\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "# Configures model to use GPU\n",
        "device = torch.device('cuda')\n",
        "model.to(torch.device(\"cuda:0\"))\n",
        "\n",
        "print('download complete')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "download complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOUeTeY2iSBU"
      },
      "source": [
        "path = \"/content/drive/My Drive/sotu\"\n",
        "dirs = os.listdir(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXwQ-ChCQqzy"
      },
      "source": [
        "# run when doing for first time\n",
        "os.mkdir(\"/content/drive/My Drive/sotuSummaries\",\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruwLaD9LSBJ9"
      },
      "source": [
        "  summaries = open(\"/content/drive/My Drive/sotuSummaries/super_summary.txt\",\"w\")\n",
        "for file in dirs:\n",
        "    reader = open(\"/content/drive/My Drive/sotu/\"+file,\"r\")\n",
        "\n",
        "    text = reader.read()\n",
        "\n",
        "\n",
        "    # Adjust text stripper to make sure text is stripped properly\n",
        "    preprocess_text = text.strip().replace(\"\\n\",\" \")\n",
        "    t5_prepared_Text = \"summarize: \"+preprocess_text\n",
        "    print (\"original text preprocessed: \\n\", preprocess_text)\n",
        "\n",
        "    # Adjust max_length for speech size based on tokens\n",
        "    tokenized_text = tokenizer.encode(t5_prepared_Text, max_length = 2700,truncation=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "\n",
        "    # summmarize \n",
        "    # Change no_repeat_ngram_size to adjust size of Ngrams in summary\n",
        "    summary_ids = model.generate(tokenized_text,\n",
        "                                        num_beams=4,\n",
        "                                        no_repeat_ngram_size=4,\n",
        "                                        min_length=30,\n",
        "                                        max_length=100,\n",
        "                                        early_stopping=True)\n",
        "\n",
        "    output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "    print (\"\\n\\nSummarized text: \\n\",output)\n",
        "    print()\n",
        "\n",
        "    original_text_pos = nlp(preprocess_text)\n",
        "    summary_pos = nlp(output)\n",
        "\n",
        "    print()\n",
        "\n",
        "    jsonStuff = {\"original_text:\": preprocess_text, \n",
        "                    \"summary_text\": output,\n",
        "                    \"compression_ratio\": str(len(output)/len(preprocess_text)),\n",
        "                    \"original_pos_tags\": Counter([token.pos_ for token in original_text_pos]),\n",
        "                    \"summary_pos_tags\": Counter([token.pos_ for token in summary_pos])\n",
        "                    }\n",
        "        \n",
        "    summaries.write(json.dumps(jsonStuff) + \"\\n\")\n",
        "  \n",
        "  reader.close()\n",
        "  summaries.close()\n",
        "\n",
        "  # Compression Ratio\n",
        "  # print (len(output)/len(text))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}